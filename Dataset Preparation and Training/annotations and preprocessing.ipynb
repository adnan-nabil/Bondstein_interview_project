{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNOTATIONS WITH LABELME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 03:25:45,576 [INFO   ] __init__:get_config:67- Loading config file from: C:\\Users\\User\\.labelmerc\n",
      "2025-02-04 04:04:18,747 [ERROR  ] label_file:load_image_file:50- Failed opening image file: G:\\rickshaw-car-motorcycle\\dataset\\022ec329-6675-44d8-922d-70af48eba824.jpg\n",
      "2025-02-04 04:04:40,353 [ERROR  ] label_file:load_image_file:50- Failed opening image file: G:\\rickshaw-car-motorcycle\\dataset\\022ec329-6675-44d8-922d-70af48eba824.jpg\n",
      "2025-02-04 04:06:18,590 [ERROR  ] label_file:load_image_file:50- Failed opening image file: G:\\rickshaw-car-motorcycle\\dataset\\022ec329-6675-44d8-922d-70af48eba824.jpg\n"
     ]
    }
   ],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGMENTATION WITH ALBUMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"G:/rickshaw-car-motorcycle/dataset\"\n",
    "annotations = \"G:/rickshaw-car-motorcycle/dataset_annotations.json\"\n",
    "output_image_folder = \"G:/rickshaw-car-motorcycle/aug_dataset\"\n",
    "output_annotations = \"G:/rickshaw-car-motorcycle/aug_dataset_annotations.json\"\n",
    "\n",
    "import json\n",
    "import albumentations as A\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "os.makedirs(output_image_folder, exist_ok=True)\n",
    "\n",
    "# Puran coco annotation load kore nilam\n",
    "with open(annotations, \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "# ki ki and kemon kemon transformation hobe set kore dilam\n",
    "# bounding box keo handle korte hobe bole dilam, format o diye dilam\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Blur(p=0.5),\n",
    "    A.Resize(224, 224),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_id']))  \n",
    "\n",
    "# notun image and annotation er list banailam and oitar moddhe puran data dhukay rakhlam\n",
    "new_images = d[\"images\"].copy()\n",
    "new_annotations = d[\"annotations\"].copy()\n",
    "\n",
    "# notun image and annotations er id number koto hobe eita bair kore nilam. max er theke 1 beshi kore barte thakbe\n",
    "image_id = max(img[\"id\"] for img in new_images) + 1\n",
    "annotation_id = max(ann[\"id\"] for ann in new_annotations) + 1  \n",
    "\n",
    "\n",
    "# ekta ekta kore image niya ashlam\n",
    "for img_info in d[\"images\"]:\n",
    "    image_path = os.path.join(image_folder, img_info[\"file_name\"])\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # image e jhamela thakle oita bad diya move on\n",
    "    if image is None:\n",
    "        print(f\"{image_path} not found\")\n",
    "        continue\n",
    "    \n",
    "    # oi image er annotation gula niye ashlam\n",
    "    image_annotations = [ann for ann in d[\"annotations\"] if ann[\"image_id\"] == img_info[\"id\"]]\n",
    "    \n",
    "    # annotation gula theke bounding box and category id bair koira nilam\n",
    "    bboxes = [ann[\"bbox\"] for ann in image_annotations]\n",
    "    category_ids = [ann[\"category_id\"] for ann in image_annotations]\n",
    "    \n",
    "    for i in range(3):\n",
    "        # dilam augmentation chalaiya\n",
    "        augmented = transform(image=image, bboxes=bboxes, category_id=category_ids)\n",
    "    \n",
    "        # augmented image er ekta notun nam dilam ar koi save hobe seita set koira dilam\n",
    "        base_name = os.path.basename(img_info[\"file_name\"])\n",
    "        new_file_name = f\"{os.path.splitext(base_name)[0]}_aug{i}.jpg\"\n",
    "        new_image_path = os.path.join(output_image_folder, new_file_name)\n",
    "    \n",
    "        # augmented image save koira dilam\n",
    "        cv2.imwrite(new_image_path, augmented[\"image\"])\n",
    "    \n",
    "        # augmented image take notun image list e dhukay dilam dorkari info shoho\n",
    "        new_images.append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": new_file_name,\n",
    "            \"height\": 224,\n",
    "            \"width\": 224\n",
    "        })\n",
    "    \n",
    "        # annotation gulakeo save koira nilam\n",
    "        # annotation er number increease koira dilam\n",
    "        # image er number o increase koira dilam\n",
    "        for bbox, category_id in zip(augmented[\"bboxes\"], category_ids):\n",
    "            new_annotations.append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": bbox\n",
    "            })\n",
    "            annotation_id += 1\n",
    "        image_id += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "# notun ekta coco format er dictionary banailam\n",
    "# augment howa image ar annotations shob eksathe eikhane joma korlam    \n",
    "new_coco_data = {\n",
    "    \"images\": new_images,\n",
    "    \"annotations\": new_annotations,\n",
    "    \"categories\": d[\"categories\"]\n",
    "}        \n",
    "\n",
    "# purata ke json e dump korlam. hoiya gelo notun dataset and annotations\n",
    "with open(output_annotations, \"w\") as f:\n",
    "    json.dump(new_coco_data, f)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"G:/rickshaw-car-motorcycle/aug_dataset\"\n",
    "annotations = \"G:/rickshaw-car-motorcycle/aug_dataset/aug_dataset_annotations.json\"\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "\n",
    "# json fie load kore nilam\n",
    "with open(annotations, \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# random shuffle kore dilam\n",
    "# 0 theke 500 porjonto train e, baki valid e    \n",
    "random.shuffle(coco_data[\"images\"])\n",
    "train_images = coco_data[\"images\"][:500]\n",
    "val_images = coco_data[\"images\"][500:]\n",
    "\n",
    "# train and valid er id gula alada kore nilam\n",
    "train_ids = {img[\"id\"] for img in train_images}\n",
    "val_ids = {img[\"id\"] for img in val_images}\n",
    "\n",
    "# train and valid er annotation gula alada kore nilam\n",
    "train_annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] in train_ids]\n",
    "val_annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] in val_ids]    \n",
    "\n",
    "# train and valid er json file banailam\n",
    "train_coco = {\"images\": train_images, \"annotations\": train_annotations, \"categories\": coco_data[\"categories\"]}\n",
    "val_coco = {\"images\": val_images, \"annotations\": val_annotations, \"categories\": coco_data[\"categories\"]}\n",
    "\n",
    "# train and valid er folder banailam\n",
    "train_folder = \"G:/rickshaw-car-motorcycle/aug_dataset/train\"\n",
    "valid_folder = \"G:/rickshaw-car-motorcycle/aug_dataset/valid\"\n",
    "# sure hoye nilam folder asholei ase to!\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(valid_folder, exist_ok=True)\n",
    "\n",
    "# json file er path save korlam\n",
    "train_json_path = os.path.join(train_folder, \"train_annotations.json\")\n",
    "val_json_path = os.path.join(valid_folder, \"valid_annotations.json\")\n",
    "\n",
    "# json file e dump kore dilam \n",
    "with open(train_json_path, \"w\") as f:\n",
    "    json.dump(train_coco, f)\n",
    "\n",
    "with open(val_json_path, \"w\") as f:\n",
    "    json.dump(val_coco, f)\n",
    "    \n",
    "    \n",
    "# image gulao copy kore relevent folder e pathay dilam\n",
    "for img in train_images:\n",
    "    file_name = os.path.basename(img[\"file_name\"])  \n",
    "    shutil.copy(os.path.join(folder, file_name), train_folder)\n",
    "\n",
    "for img in val_images:\n",
    "    file_name = os.path.basename(img[\"file_name\"])  \n",
    "    shutil.copy(os.path.join(folder, file_name), valid_folder)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "COCO to YOLO conversion complete for G:/rickshaw-car-motorcycle/aug_dataset/train/train_annotations.json.\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "COCO to YOLO conversion complete for G:/rickshaw-car-motorcycle/aug_dataset/valid/valid_annotations.json.\n",
      "COCO to YOLO conversion complete for both train and valid datasets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import shutil\n",
    "\n",
    "# shob dorkari path gula niye nilam\n",
    "train_annotation_file = 'G:/rickshaw-car-motorcycle/aug_dataset/train/train_annotations.json'\n",
    "valid_annotation_file = 'G:/rickshaw-car-motorcycle/aug_dataset/valid/valid_annotations.json'\n",
    "train_image_dir = 'G:/rickshaw-car-motorcycle/aug_dataset/train' \n",
    "valid_image_dir = 'G:/rickshaw-car-motorcycle/aug_dataset/valid'  \n",
    "\n",
    "\n",
    "output_train_images_dir = 'G:/rickshaw-car-motorcycle/aug_dataset/yolo-train/images'\n",
    "output_train_labels_dir = 'G:/rickshaw-car-motorcycle/aug_dataset/yolo-train/labels'\n",
    "output_valid_images_dir = 'G:/rickshaw-car-motorcycle/aug_dataset/yolo-valid/images'\n",
    "output_valid_labels_dir = 'G:/rickshaw-car-motorcycle/aug_dataset/yolo-valid/labels'\n",
    "\n",
    "# folder gula asholei ase to !\n",
    "os.makedirs(output_train_images_dir, exist_ok=True)\n",
    "os.makedirs(output_train_labels_dir, exist_ok=True)\n",
    "os.makedirs(output_valid_images_dir, exist_ok=True)\n",
    "os.makedirs(output_valid_labels_dir, exist_ok=True)\n",
    "\n",
    "# conversion function \n",
    "def convert_coco_to_yolo(annotation_file, image_dir, output_images_dir, output_labels_dir):\n",
    "   \n",
    "    coco = COCO(annotation_file)\n",
    "    \n",
    "    # category gula load kore nilam\n",
    "    # class name and id ke ekta dictionary te raikha dilam\n",
    "    categories = coco.loadCats(coco.getCatIds())\n",
    "    class_names = [category['name'] for category in categories]\n",
    "    class_dict = {category['id']: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "    # image gula load koira nilam\n",
    "    image_ids = coco.getImgIds()\n",
    "    for image_id in image_ids:\n",
    "        image_info = coco.loadImgs(image_id)[0]\n",
    "        image_filename = image_info['file_name']\n",
    "\n",
    "        # image er path banailam\n",
    "        image_path = os.path.join(image_dir, os.path.basename(image_filename))\n",
    "\n",
    "        # image na thakle skip kore chole jabe and warning show kore dibe\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: {image_path} does not exist. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        basename = os.path.basename(image_filename)\n",
    "\n",
    "        # image copy koira dilam\n",
    "        shutil.copy(image_path, os.path.join(output_images_dir, basename))\n",
    "\n",
    "        # annotation gula load koira nilam\n",
    "        annotation_ids = coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "        # label file er path banailam\n",
    "        label_filename = basename.replace('.jpg', '.txt')\n",
    "        label_filepath = os.path.join(output_labels_dir, label_filename)\n",
    "\n",
    "        # label file e data write koira dilam\n",
    "        with open(label_filepath, 'w') as label_file:\n",
    "            for annotation in annotations:\n",
    "                \n",
    "                # bounding box er center, width, height calculate koira dilam\n",
    "                bbox = annotation['bbox']\n",
    "                x_center = (bbox[0] + bbox[2] / 2) / image_info['width']\n",
    "                y_center = (bbox[1] + bbox[3] / 2) / image_info['height']\n",
    "                width = bbox[2] / image_info['width']\n",
    "                height = bbox[3] / image_info['height']\n",
    "\n",
    "                # class id and bounding box er data write koira dilam\n",
    "                category_id = annotation['category_id']\n",
    "                if category_id in class_dict:\n",
    "                    label_file.write(f\"{class_dict[category_id]} {x_center} {y_center} {width} {height}\\n\")\n",
    "                else:\n",
    "                    print(f\"Warning: Category ID {category_id} not found in class_dict\")\n",
    "\n",
    "    print(f\"COCO to YOLO conversion complete for {annotation_file}.\")\n",
    "\n",
    "\n",
    "convert_coco_to_yolo(train_annotation_file, train_image_dir, output_train_images_dir, output_train_labels_dir)\n",
    "convert_coco_to_yolo(valid_annotation_file, valid_image_dir, output_valid_images_dir, output_valid_labels_dir)\n",
    "\n",
    "print(\"COCO to YOLO conversion complete for both train and valid datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model. Kaggle notebook is available in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.71 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.34  Python-3.10.14 torch-2.5.1 CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=G:/rickshaw-car-motorcycle/data.yaml, epochs=2, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,136,761 parameters, 11,136,745 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning G:\\rickshaw-car-motorcycle\\yolo-dataset\\train\\labels... 500 images, 0 backgrounds, 0 corrupt: 100%|██████████| 500/500 [00:01<00:00, 393.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: G:\\rickshaw-car-motorcycle\\yolo-dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\User\\miniconda3\\envs\\acienv\\lib\\site-packages\\ultralytics\\data\\augment.py:1850: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
      "  A.ImageCompression(quality_lower=75, p=0.0),\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\rickshaw-car-motorcycle\\yolo-dataset\\valid\\labels... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00<00:00, 394.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: G:\\rickshaw-car-motorcycle\\yolo-dataset\\valid\\labels.cache\n",
      "Plotting labels to runs\\detect\\train3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train3\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2         0G      1.671      2.258      1.565         25        640: 100%|██████████| 32/32 [14:43<00:00, 27.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:08<00:00, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        300      0.452      0.238      0.225      0.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2         0G      1.458       1.88      1.479         70        640:   3%|▎         | 1/32 [01:16<39:37, 76.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\acienv\\lib\\site-packages\\ultralytics\\engine\\model.py:802\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 802\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\acienv\\lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\acienv\\lib\\site-packages\\ultralytics\\engine\\trainer.py:388\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    384\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\acienv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\acienv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\acienv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = 'yolov8s.pt'\n",
    "data_path = 'G:/rickshaw-car-motorcycle/data.yaml'\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "\n",
    "lr0 = 0.001 \n",
    "lrf = 0.1\n",
    "batch = 16 \n",
    "epochs = 2\n",
    "optimizer = 'Adam'  \n",
    "\n",
    "\n",
    "model.train(data=data_path, epochs=epochs, imgsz=640, batch=batch, lr0=lr0, lrf=lrf, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
